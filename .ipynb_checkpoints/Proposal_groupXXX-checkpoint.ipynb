{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Allen Phu\n",
    "- Kevin Yu\n",
    "- Saksham Rai\n",
    "- Rodrigo Lizaran-Molina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Being provided with a dataset of many characters in American Sign Language, we thought it would be interesting to utilize many of the clustering techniques taught in this course to generate clusters of the ASL characters and from there, have our models be optimized enough to be able to use our web camera and create our own signs, the model should be able to correctly assign it to a cluster and return the character. Our data is represented as a vector of pixel intensities that range from 0-255 (single dimensional as it is greyscale). With this, we would run a clustering algorithm on the greyscale images. Next, assuming the clusters are accurate, we would transform our webcam image with a student forming a sign, and input the greyscale image into our clustering algorithm where it would then identify the proper character. To measure performance, we would allocate a portion of the dataset for testing and based off the proportion of correct classifications vs incorrect classifications, we would return an accuracy percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Our group originally came across Google’s “American Sign Language Fingerspelling Recognition” Kaggle competition <a name=\"one\"></a>[<sup>[1]</sup>](#onenote) while brainstorming for ideas for our project. We were intrigued by this idea as we were all interested in ML image processing in the first place, but the combination of making advancements in accessibility and image processing only solidified this topic as something we wanted to pursue for our final project. \n",
    "\n",
    "\n",
    "After some further research, we realized that although we were intrigued by the idea of mixing ML and ASL together, only one of us had prior experience with the MediaPipe library. As many prior writeups regarding the utilization of ML utilized the aforementioned MediaPipe library <a name=\"two\"></a>[<sup>[2]</sup>](#twonote) <a name=\"three\"></a>[<sup>[3]</sup>](#threenote), we decided to pivot towards the Sign Language MNIST dataset <a name=\"four\"></a>[<sup>[4]</sup>](#fournote) in order to make the project more digestible for ourselves. Previous studies have shown that using image recognition platforms in order to recognize ASL have already been successful, with an October 2023 study achieving 98.98% test accuracy <a name=\"five\"></a>[<sup>[5]</sup>](#fivenote) and an August 2022 study utilizing MediaPipe, Keras, and the Sign Language MNIST dataset achieving 95% training accuracy <a name=\"three\"></a>[<sup>[3]</sup>](#threenote). \n",
    "\n",
    "\n",
    "Regarding advancements in what’s been done for machine learning and ASL detection, Google has launched Project Shuwa in the past in order to bring awareness and teach more people about ASL <a name=\"two\"></a>[<sup>[2]</sup>](#twonote). One (of many components) of Project Shuwa is SignTown, an “interactive game that utilizes webcams and a web browser to help people learn about sign language and Deaf culture” <a name=\"two\"></a>[<sup>[2]</sup>](#twonote). Google has also made it easier to learn about both ASL and machine learning through the utilization of their “Teachable Machine” tool, where people can use a no-code approach to leverage machine learning to test a model’s ability to recognize ASL samples <a name=\"six\"></a>[<sup>[6]</sup>](#sixnote).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are aiming to solve is the classification of ASL characters to provide an ability of communication from deaf people to those who do not understand ASL. With the 27,455 available training data samples, and 7172 samples allocated for testing, we can quantify the success rate by taking the proportion of correct classifications against incorrect classifications. Additionally, we plan to test our clusters by inputting test cases from making the signs on our webcams and observing if the results share a similar accuracy with the test data. This can be replicated as with the publicly available data, one can follow our methods of creating clusters for the data as well as our process in inputting the data from our webcamera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- Our data will be composed of hand images. Each image will represent a letter of the American Sign Language.\n",
    "- Sign_mnist_test and Sign_mnist_train\n",
    "    - [Link](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data)\n",
    "    - 1570 variables/columns (785 each one) with 27455 observations in training data and 7172 observations in test data.\n",
    "    - Each of the 27455 observations represent an image and it is paired with a corresponding label on what hand sign the sample represents\n",
    "    - The images are represented as 784 pixels and each pixel ranges from 0-255 and these images are represented as greyscale versions of themselves\n",
    "    - There will not be any special handling/transformations for this data as we can immediately begin clustering with the numeric data values stored in each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To solve the problem of clustering ASL (American Sign Language) hand sign images using unsupervised machine learning and achieving high accuracy in real-time sign language interpretation through a camera, we can employ the following solution:\n",
    "\n",
    "***Solution Description***\n",
    "\n",
    "1. Preprocessing:\n",
    "    - Resize images to a consistent dimension to ensure uniformity.\n",
    "    - Transform pixel values into 1D array for each image.\n",
    "\n",
    "2. Feature Extraction:\n",
    "    - Utilize a dimensionality reduction technique such as PCA, to extract essential features and reduce computational complexity.\n",
    "\n",
    "3. Unsupervised Clustering:\n",
    "    - Apply clustering algortihm such as k-means, DBSCAN to group similar hand sign images together. (The choice of k can be determined by an Elbow method or Silhouette Score)\n",
    "\n",
    "4. Cluster Label Assigment:\n",
    "    - Assign labels to the clusters based on the majority class of the images within each cluster.\n",
    "\n",
    "5. Real-time Sign Language Interpretation:\n",
    "    - Use a camera to capture frames in real-time.\n",
    "    - Preprocess each frame similarly to the training data.\n",
    "    - Apply the trained clustering model to assign a cluster label to the current frame.\n",
    "    - Map the cluster label to the corresponding ASL letter using the assigned labels from the training phase.\n",
    "\n",
    "6. Testing and Validation:\n",
    "    - Split the dataset into training and testing sets to evaluate the model's performance.\n",
    "    - Use metrics like accuracy, precision, recall, and F1-score to assess the clustering model's performance on the testing set.\n",
    "    - For real-time interpretation, validate the accuracy by comparing the predicted letters to the ground truth ASL letters in a controlled environment.\n",
    "\n",
    "7. Benchmark Model:\n",
    "    - Google open Teachable Machine<a name=\"MLTeaching\"></a>[<sup>[7]</sup>](#MLTeachingNote) allow to upload a video or photos of our hands making the ASL, it will train the model and print out accuracy of the model.\n",
    "\n",
    "\n",
    "***Implementation Details***\n",
    "- Libraries: \n",
    "    - Use popular machine learning libraries such as scikit-learn for clustering algorithms, OpenCV for image processing, and NumPy for numerical operations.\n",
    "\n",
    "- Function Calls:\n",
    "    - Use scikit-learn's implementation of K-means or other clustering algorithms.\n",
    "    - Apply PCA for dimensionality reduction using scikit-learn's PCA module.\n",
    "\n",
    "- Real-time Integration:\n",
    "    - Utilize OpenCV for capturing and processing camera frames.\n",
    "    - Implement a real-time loop to continuously process frames and display the predicted ASL letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since our project mainly focuses on clustering algorithms to identify similar features between different ASL hand sign images, we need to check how efficient our algorithm is in forming these clusters, and how distinct they are from other clusters. To achieve this, the best feature to use is the Adjusted Rand Score. Adjusted Rand Score is a measure of similarity between two clusterings, particularly focusing on pairs that are in agreement - belonging either to different or the same clusters. The score is a ratio of the number of agreeing pairs to the total number of pairs, giving a value between 0 (no agreement) and 1 (perfect agreement). In contrast to the Rand Score, the Adjusted Rand Score, as the name suggests “adjusts” the random score by accounting for clustering by chance. It does this by assigning close to 0 values to random clusterings and 1 for perfect matches. \n",
    "\n",
    "Beyond this, we will also test the model using traditional classification metrics in an adapted manner. After clustering, we assign the most common true label within each cluster to all its points, enabling us to apply metrics like accuracy, precision, recall, and the F1 score. This will be done to test the model for its accuracy in producing true and false positives/negatives. \n",
    "\n",
    "In addition to this, we will test our model's accuracy to handle real-time hand-sign interpretations. Non-informative ASL communicators will stand in front of the webcam and make one of the many signs. The model will predict the hand sign. This can be done by displaying the model's prediction on a screen in real time or logging the results for later analysis. We will compare the model's real-time predictions with the actual signs being performed to assess accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our American Sign Language recognition research project utilises the MNIST dataset of hand signs. Even though this dataset has been fairly corroborated and extensively updated over the years; since our project has an application in helping the \"hard of hearing\", it's necessary to consider some possible ethical concerns:\n",
    "\n",
    "1. Clustering Algorithm Bias: The MNIST training data might lack diversity in hand shapes, skin tones, signing styles, or individual variations, resulting in the algorithm prioritising majority patterns, leading to inaccurate classifications for underrepresented groups. For instance, a dataset skewed towards younger signers might struggle with recognizing signs used more frequently by older adults. In addition to this, even with a perfect algorithm also, there might be implicit biases in the dataset which are being amplified by our clustering algorithm.For example, the data might contain more examples of right-handed signing, potentially impacting the model's ability to recognize signs performed with the left hand. While such conclusions are less often, they are important to take under consideration because the implications of such biases are significant. Misclassifications can lead to communication breakdowns, and misunderstandings, all of which on an extended period and a larger sample space may even lead to social exclusion for specific groups within the Deaf community. We aim to address these concerns by trialling and testing the usage of different clustering algorithms to mitigate the risk of false clustering. Hierarchical clustering, for instance, builds a hierarchy of clusters, allowing me to zoom in on specific signing styles or hand shapes potentially underrepresented in the data. Similarly, density-based methods like DBSCAN focus on identifying clusters based on data density, making them less susceptible to the influence of majority patterns that could amplify bias. By evaluating these alternative algorithms with diverse test sets and fairness metrics (accuracy, precision and F1 score), we will choose the model that best balances inclusivity and accuracy, ensuring equitable representation across all user groups.\n",
    "\n",
    "2. Data Privacy Issue: The MNIST Hand Sign dataset we are utilising contains 27,455 cases of test data. These are real-life pictures collected from different thousands of different people. Now, in high-security stakes, hand-pictures also serve as identifiers. Due to this reason, similar to any other dataset, our MNIST ASL dataset is also subject to the protection of the privacy of individuals. In addition to this, we will also have input data from voluntary non-ASL communicators to test the real-time interpretation feature of the app. We hope to address this by keeping the identity of the volunteers undisclosed. In addition, we do it by utilising a \"black-box\" way of allowing the user to interact with the model. Without giving them any information about existing ASL hand signs, we hope to test the model for its accuracy in identifying any or all signs that people make, closest to an actual ASL sign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a team, we are committed to maintaining a high level of collaboration, professionalism, and respect among all members. Our shared expectations for one another are as follows:\n",
    "\n",
    "* **Active and Respectful Communication**: We value open and active communication. Team members should feel comfortable expressing their thoughts, ideas, and concerns. We will listen actively and respectfully to each other.\n",
    "\n",
    "* **Idea Exchange**: Everyone is welcome to contribute their thoughts and suggestions. We will consider each other's ideas with respect and appreciation.\n",
    "\n",
    "* **Continuous Collaboration**: If any team member has ideas or encounters challenges, please share them directly through our Discord communication channel as soon as possible. We believe that open and timely communication is essential to address issues promptly and efficiently.\n",
    "\n",
    "* **Timely Task Completion**: We understand the importance of meeting deadlines. Team members are expected to complete their assigned sections on time. If you foresee challenges in meeting a deadline, please communicate this at least two days in advance so that we can collectively find solutions.\n",
    "\n",
    "* **Equal Work Distribution**: The workload will be distributed equitably. All team members are expected to contribute to the final project, ensuring that no one bears an undue burden.\n",
    "\n",
    "\n",
    "\n",
    "**Rodrigo Lizaran-Molina**: Data, Proposed Solution\n",
    "\n",
    "**Allen Phu**: Background\n",
    "\n",
    "**Kevin Y**: Abstract, Data and Problem Statement\n",
    "\n",
    "**Saksham Rai**: Evaluation Metrics, Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/18  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/19  |  4 PM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/20  |  4 PM | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/25  |  6 PM | Import & Wrangle Data ,do some EDA  | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 3/1   | 12 PM | Finalize wrangling/EDA; Begin programming for project  | Discuss/edit project code; Complete project |\n",
    "| 3/19  | 12 PM | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"onenote\"></a>1.[^](#one): Ashley Chow, Glenn Cameron, Manfred Georg, Mark Sherwood, Phil Culliton, Sam Sepah, Sohier Dane, Thad Starner. (2023). Google - American Sign Language Fingerspelling Recognition. Kaggle. https://kaggle.com/competitions/asl-fingerspelling<br> \n",
    "\n",
    "<a name=\"twonote\"></a>2.[^](#two): El Moujahid, K. (2021, December 1). Machine learning to make sign language more accessible. Google. https://blog.google/outreach-initiatives/accessibility/ml-making-sign-language-more-accessible/<br> \n",
    "\n",
    "<a name=\"threenote\"></a>3.[^](#three): Garimella, M. (2022, August 23). Sign Language Recognition with Advanced Computer Vision. Medium. https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442<br> \n",
    "\n",
    "<a name=\"fournote\"></a>4.[^](#four): tecperson. (October 2017). Sign Language MNIST. Kaggle. https://www.kaggle.com/datasets/datamunge/sign-language-mnist<br> \n",
    "\n",
    "<a name=\"fivenote\"></a>5.[^](#five): Pathan, R. K., Biswas, M., Yasmin, S., Khandaker, M. U., Salman, M., & Youssef, A. A. F. (2023). Sign language recognition using the fusion of image and hand landmarks through multi-headed convolutional neural network. Scientific Reports, 13(1), 16975. https://doi.org/10.1038/s41598-023-43852-x<br> \n",
    "\n",
    "<a name=\"sixnote\"></a>6.[^](#six): Chen, Y. (2023, December 29). Learning American Sign Language (ASL) with Google’s Teachable Machine: A No-Code Experiment. Medium. https://medium.com/@dynotes/breaking-barriers-using-googles-no-code-approach-for-sign-language-recognition-and-learning-fc92ae16522c#bypass<br> \n",
    "\n",
    "\"<a name=\"MLTeaching\"></a>7.[^](#MLTeachingNote) Google open Teachable Machine. https://teachablemachine.withgoogle.com/train/tiny_image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
